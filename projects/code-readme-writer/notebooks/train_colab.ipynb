{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code README Writer â€” SFT Training on Google Colab\n",
    "\n",
    "Fine-tune TinyLlama-1.1B with QLoRA to generate README.md files from repository structure and code.\n",
    "\n",
    "**Requirements:** Free Colab T4 GPU (~2-3 hours training time)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers datasets peft trl bitsandbytes accelerate\n",
    "!pip install -q sentencepiece protobuf wandb rouge-score gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load & Prepare Dataset\n",
    "\n",
    "We use pre-collected GitHub repo-README pairs. Each example contains:\n",
    "- `repo_name`: The repository identifier\n",
    "- `file_tree`: Directory structure listing\n",
    "- `code_snippets`: Contents of key source files\n",
    "- `readme_content`: The ground-truth README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Load from HuggingFace Hub or local file\n",
    "# Option A: From local JSONL (upload readme_dataset.jsonl to Colab)\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # upload readme_dataset.jsonl\n",
    "\n",
    "# Option B: From HuggingFace Hub (after pushing dataset)\n",
    "# dataset = load_dataset(\"your-username/code-readme-pairs\")\n",
    "\n",
    "# For this notebook, we'll create a small demo dataset\n",
    "demo_examples = [\n",
    "    {\n",
    "        \"repo_name\": \"fastapi-todo-app\",\n",
    "        \"file_tree\": \"app/\\napp/__init__.py\\napp/main.py\\napp/models.py\\napp/database.py\\nrequirements.txt\\nDockerfile\",\n",
    "        \"code_snippets\": \"--- requirements.txt ---\\nfastapi==0.104.1\\nuvicorn==0.24.0\\nsqlalchemy==2.0.23\\n\\n--- app/main.py ---\\nfrom fastapi import FastAPI\\napp = FastAPI(title='Todo API')\\n\",\n",
    "        \"readme_content\": \"# FastAPI Todo App\\n\\nA RESTful todo API built with FastAPI and SQLAlchemy.\\n\\n## Installation\\n\\n```bash\\npip install -r requirements.txt\\n```\\n\\n## Usage\\n\\n```bash\\nuvicorn app.main:app --reload\\n```\\n\\n## Features\\n\\n- CRUD operations for todos\\n- SQLAlchemy ORM\\n- Docker support\\n\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# In practice, load your full dataset:\n",
    "# examples = []\n",
    "# with open('readme_dataset.jsonl') as f:\n",
    "#     for line in f:\n",
    "#         examples.append(json.loads(line))\n",
    "\n",
    "examples = demo_examples  # Replace with full dataset\n",
    "dataset = Dataset.from_list(examples)\n",
    "split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "dataset_dict = DatasetDict({\"train\": split[\"train\"], \"test\": split[\"test\"]})\n",
    "\n",
    "print(f\"Train: {len(dataset_dict['train'])} examples\")\n",
    "print(f\"Test: {len(dataset_dict['test'])} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Base Model with QLoRA (4-bit Quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# 4-bit quantization config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Model size: {model.num_parameters() / 1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate BEFORE Example (Base Model)\n",
    "\n",
    "Let's see what the base model produces before fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(repo_name, file_tree, code_snippets):\n",
    "    return (\n",
    "        \"<|system|>\\n\"\n",
    "        \"You are a technical writer that generates comprehensive README.md files \"\n",
    "        \"for code repositories. Given the repository structure and code contents, \"\n",
    "        \"write a clear, well-structured README.</s>\\n\"\n",
    "        \"<|user|>\\n\"\n",
    "        \"Generate a README.md for the following repository:\\n\\n\"\n",
    "        f\"Repository name: {repo_name}\\n\\n\"\n",
    "        f\"File structure:\\n{file_tree}\\n\\n\"\n",
    "        f\"Key files:\\n{code_snippets}</s>\\n\"\n",
    "        \"<|assistant|>\\n\"\n",
    "    )\n",
    "\n",
    "def generate_readme(model, prompt, max_new_tokens=512):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7, top_p=0.9, repetition_penalty=1.15,\n",
    "            do_sample=True, pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    generated = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
    "\n",
    "# Test with example\n",
    "test_example = examples[0]\n",
    "prompt = build_prompt(test_example['repo_name'], test_example['file_tree'], test_example['code_snippets'])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BEFORE FINE-TUNING (Base Model Output)\")\n",
    "print(\"=\" * 60)\n",
    "base_output = generate_readme(model, prompt)\n",
    "print(base_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train with SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# Prompt template\n",
    "TEMPLATE = \"\"\"<|system|>\n",
    "You are a technical writer that generates comprehensive README.md files for code repositories. Given the repository structure and code contents, write a clear, well-structured README.</s>\n",
    "<|user|>\n",
    "Generate a README.md for the following repository:\n",
    "\n",
    "Repository name: {repo_name}\n",
    "\n",
    "File structure:\n",
    "{file_tree}\n",
    "\n",
    "Key files:\n",
    "{code_snippets}</s>\n",
    "<|assistant|>\n",
    "{readme_content}</s>\"\"\"\n",
    "\n",
    "def formatting_func(examples):\n",
    "    outputs = []\n",
    "    for i in range(len(examples['repo_name'])):\n",
    "        text = TEMPLATE.format(\n",
    "            repo_name=examples['repo_name'][i],\n",
    "            file_tree=examples['file_tree'][i],\n",
    "            code_snippets=examples['code_snippets'][i],\n",
    "            readme_content=examples['readme_content'][i],\n",
    "        )\n",
    "        outputs.append(text)\n",
    "    return outputs\n",
    "\n",
    "# Only compute loss on assistant response\n",
    "response_template = \"<|assistant|>\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Training arguments (tuned for Colab T4)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./readme-writer-checkpoints\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    max_grad_norm=0.3,\n",
    "    group_by_length=True,\n",
    "    report_to=\"none\",  # Set to \"wandb\" for experiment tracking\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict['train'],\n",
    "    eval_dataset=dataset_dict['test'],\n",
    "    formatting_func=formatting_func,\n",
    "    data_collator=collator,\n",
    "    max_seq_length=2048,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned adapter\n",
    "ADAPTER_PATH = \"./readme-writer-adapter\"\n",
    "trainer.save_model(ADAPTER_PATH)\n",
    "tokenizer.save_pretrained(ADAPTER_PATH)\n",
    "print(f\"Adapter saved to {ADAPTER_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate AFTER Example (Fine-tuned Model)\n",
    "\n",
    "Now let's generate with the same prompt and compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"AFTER FINE-TUNING (SFT + QLoRA Output)\")\n",
    "print(\"=\" * 60)\n",
    "finetuned_output = generate_readme(model, prompt)\n",
    "print(finetuned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SIDE-BY-SIDE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n--- BEFORE (Base Model) ---\")\n",
    "print(f\"Length: {len(base_output)} chars\")\n",
    "print(f\"Headings: {base_output.count('#')}\")\n",
    "print(f\"Code blocks: {base_output.count('```') // 2}\")\n",
    "print()\n",
    "print(base_output[:500])\n",
    "\n",
    "print(\"\\n--- AFTER (Fine-tuned) ---\")\n",
    "print(f\"Length: {len(finetuned_output)} chars\")\n",
    "print(f\"Headings: {finetuned_output.count('#')}\")\n",
    "print(f\"Code blocks: {finetuned_output.count('```') // 2}\")\n",
    "print()\n",
    "print(finetuned_output[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate with ROUGE Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "reference = test_example['readme_content']\n",
    "\n",
    "base_scores = scorer.score(reference, base_output)\n",
    "ft_scores = scorer.score(reference, finetuned_output)\n",
    "\n",
    "print(f\"{'Metric':<10} {'Base':>10} {'Fine-tuned':>12} {'Improvement':>14}\")\n",
    "print(\"-\" * 50)\n",
    "for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
    "    base_f = base_scores[metric].fmeasure\n",
    "    ft_f = ft_scores[metric].fmeasure\n",
    "    improvement = ((ft_f - base_f) / max(base_f, 0.001)) * 100\n",
    "    print(f\"{metric:<10} {base_f:>10.4f} {ft_f:>12.4f} {improvement:>+13.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Push to HuggingFace Hub (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and fill in to push your adapter to HuggingFace\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"your-hf-token\")\n",
    "#\n",
    "# model.push_to_hub(\"your-username/code-readme-writer-tinyllama-lora\")\n",
    "# tokenizer.push_to_hub(\"your-username/code-readme-writer-tinyllama-lora\")\n",
    "# print(\"Adapter pushed to HuggingFace Hub!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Demo\n",
    "\n",
    "Run the cell below to launch an interactive Gradio demo right in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def generate_for_demo(repo_name, file_tree, code_snippets):\n",
    "    prompt = build_prompt(repo_name, file_tree, code_snippets)\n",
    "    return generate_readme(model, prompt, max_new_tokens=1024)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate_for_demo,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Repository Name\", value=\"my-project\"),\n",
    "        gr.Textbox(label=\"File Structure\", lines=8, value=\"src/\\nsrc/main.py\\ntests/\\nrequirements.txt\"),\n",
    "        gr.Textbox(label=\"Key Code Files\", lines=10, value=\"--- src/main.py ---\\nprint('hello')\"),\n",
    "    ],\n",
    "    outputs=gr.Markdown(label=\"Generated README\"),\n",
    "    title=\"Code README Writer\",\n",
    "    description=\"Generate README.md files from repository structure using a fine-tuned TinyLlama model.\",\n",
    ")\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
